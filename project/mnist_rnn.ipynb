{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyper parameters\n",
    "n_epochs = 10           # number of epochs to train the model\n",
    "batch_size = 64\n",
    "time_step = 28           # rnn time step / image height\n",
    "input_size = 28          # rnn input size / image width\n",
    "lr = 0.001               # learning rate\n",
    "DOWNLOAD_MNIST = True    # set to True if you have not downloaded the data before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist digital dataset\n",
    "train_data = dsets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                         # this is training data\n",
    "    transform=transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                        # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,            # download MNIST dataset if not done before\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot one example\n",
    "print(train_data.train_data.size())     # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())   # (60000)\n",
    "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for easy mini-batch return in training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# convert test data into Variable, pick 2000 samples for testing\n",
    "test_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())\n",
    "test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255.   # shape (2000, 28, 28) value in range(0,1)\n",
    "test_y = test_data.test_labels.numpy()[:2000]    # covert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(28, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # nn.RNN(), it hardly learns\n",
    "            input_size=input_size,\n",
    "            hidden_size=64,         # rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will have batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "rnn = RNN()\n",
    "print(rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.3142 | test accuracy: 0.09\n",
      "Epoch:  0 | train loss: 2.1194 | test accuracy: 0.27\n",
      "Epoch:  0 | train loss: 1.5926 | test accuracy: 0.45\n",
      "Epoch:  0 | train loss: 0.9878 | test accuracy: 0.65\n",
      "Epoch:  0 | train loss: 0.7026 | test accuracy: 0.69\n",
      "Epoch:  0 | train loss: 0.5381 | test accuracy: 0.74\n",
      "Epoch:  0 | train loss: 0.5895 | test accuracy: 0.78\n",
      "Epoch:  0 | train loss: 0.5371 | test accuracy: 0.80\n",
      "Epoch:  0 | train loss: 0.4481 | test accuracy: 0.84\n",
      "Epoch:  0 | train loss: 0.3327 | test accuracy: 0.85\n",
      "Epoch:  0 | train loss: 0.3420 | test accuracy: 0.85\n",
      "Epoch:  0 | train loss: 0.4149 | test accuracy: 0.88\n",
      "Epoch:  0 | train loss: 0.3539 | test accuracy: 0.89\n",
      "Epoch:  0 | train loss: 0.3617 | test accuracy: 0.89\n",
      "Epoch:  0 | train loss: 0.3406 | test accuracy: 0.90\n",
      "Epoch:  0 | train loss: 0.3589 | test accuracy: 0.90\n",
      "Epoch:  0 | train loss: 0.2222 | test accuracy: 0.92\n",
      "Epoch:  0 | train loss: 0.3079 | test accuracy: 0.89\n",
      "Epoch:  0 | train loss: 0.2276 | test accuracy: 0.92\n",
      "Epoch:  1 | train loss: 0.3173 | test accuracy: 0.91\n",
      "Epoch:  1 | train loss: 0.2818 | test accuracy: 0.93\n",
      "Epoch:  1 | train loss: 0.2725 | test accuracy: 0.93\n",
      "Epoch:  1 | train loss: 0.1994 | test accuracy: 0.93\n",
      "Epoch:  1 | train loss: 0.3210 | test accuracy: 0.93\n",
      "Epoch:  1 | train loss: 0.1103 | test accuracy: 0.94\n",
      "Epoch:  1 | train loss: 0.1176 | test accuracy: 0.94\n",
      "Epoch:  1 | train loss: 0.1016 | test accuracy: 0.94\n",
      "Epoch:  1 | train loss: 0.1587 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0785 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.2624 | test accuracy: 0.94\n",
      "Epoch:  1 | train loss: 0.1461 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.0701 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.2470 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.2251 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.1372 | test accuracy: 0.95\n",
      "Epoch:  1 | train loss: 0.1387 | test accuracy: 0.94\n",
      "Epoch:  1 | train loss: 0.1561 | test accuracy: 0.96\n",
      "Epoch:  1 | train loss: 0.0764 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.1826 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.1099 | test accuracy: 0.95\n",
      "Epoch:  2 | train loss: 0.1090 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.0527 | test accuracy: 0.95\n",
      "Epoch:  2 | train loss: 0.2860 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.0863 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.1651 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.0202 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.1228 | test accuracy: 0.95\n",
      "Epoch:  2 | train loss: 0.1757 | test accuracy: 0.95\n",
      "Epoch:  2 | train loss: 0.1102 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.0780 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.2029 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.2370 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.1114 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.1021 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.1062 | test accuracy: 0.96\n",
      "Epoch:  2 | train loss: 0.0625 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.2032 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0855 | test accuracy: 0.95\n",
      "Epoch:  3 | train loss: 0.1761 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0781 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0324 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0527 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0493 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0278 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.1068 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0588 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0934 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0201 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0175 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0175 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0172 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0305 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.1282 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0653 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.1036 | test accuracy: 0.96\n",
      "Epoch:  3 | train loss: 0.0461 | test accuracy: 0.96\n",
      "Epoch:  4 | train loss: 0.0975 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0437 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.1065 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.1123 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0552 | test accuracy: 0.96\n",
      "Epoch:  4 | train loss: 0.0241 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0252 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0766 | test accuracy: 0.96\n",
      "Epoch:  4 | train loss: 0.0607 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0837 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0109 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0711 | test accuracy: 0.96\n",
      "Epoch:  4 | train loss: 0.0726 | test accuracy: 0.96\n",
      "Epoch:  4 | train loss: 0.1356 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0190 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0569 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0745 | test accuracy: 0.96\n",
      "Epoch:  4 | train loss: 0.0268 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.1964 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0300 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0101 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0180 | test accuracy: 0.96\n",
      "Epoch:  5 | train loss: 0.0444 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0239 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0293 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0783 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.2001 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0424 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0494 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0538 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0653 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0278 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.1445 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0639 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.1006 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0129 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0313 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0957 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.1066 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0208 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0830 | test accuracy: 0.96\n",
      "Epoch:  6 | train loss: 0.0814 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0552 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0103 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0277 | test accuracy: 0.98\n",
      "Epoch:  6 | train loss: 0.1674 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0135 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0205 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0648 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0649 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.1333 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.1910 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0367 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0244 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.1219 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0514 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0172 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0207 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0134 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0552 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0203 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0473 | test accuracy: 0.98\n",
      "Epoch:  7 | train loss: 0.0995 | test accuracy: 0.98\n",
      "Epoch:  7 | train loss: 0.0274 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0096 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0579 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0086 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0070 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0126 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0162 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0076 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0254 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0513 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.1796 | test accuracy: 0.98\n",
      "Epoch:  7 | train loss: 0.1609 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0722 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.1428 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0031 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0168 | test accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8 | train loss: 0.0060 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0214 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0257 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0221 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0355 | test accuracy: 0.96\n",
      "Epoch:  8 | train loss: 0.0154 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0331 | test accuracy: 0.98\n",
      "Epoch:  8 | train loss: 0.0456 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0264 | test accuracy: 0.98\n",
      "Epoch:  8 | train loss: 0.0679 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0769 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.2701 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0100 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0133 | test accuracy: 0.98\n",
      "Epoch:  8 | train loss: 0.0555 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.1168 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.1501 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0482 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0200 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0983 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0518 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0067 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.1353 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.1025 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0150 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.0303 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0041 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.0069 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.0687 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0064 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.1103 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0084 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0161 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.0048 | test accuracy: 0.98\n",
      "Epoch:  9 | train loss: 0.0539 | test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# training and testing\n",
    "for epoch in range(n_epochs):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):        # gives batch data\n",
    "        b_x = b_x.view(-1, 28, 28)              # reshape x to (batch, time_step, input_size)\n",
    "\n",
    "        output = rnn(b_x)                               # rnn output\n",
    "        loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "        optimizer.zero_grad()                           # clear gradients for this training step\n",
    "        loss.backward()                                 # backpropagation, compute gradients\n",
    "        optimizer.step()                                # apply gradients\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            test_output = rnn(test_x)                   # (samples, time_step, input_size)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9] Prediction\n",
      "[7 2 1 0 4 1 4 9 5 9] Truth\n"
     ]
    }
   ],
   "source": [
    "# print 10 predictions from test data\n",
    "test_output = rnn(test_x[:10].view(-1, 28, 28))\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "print(pred_y, 'Prediction')\n",
    "print(test_y[:10], 'Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (iapr-2020)",
   "language": "python",
   "name": "pycharm-7051ba19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
